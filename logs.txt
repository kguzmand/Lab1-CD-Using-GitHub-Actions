
==> Audit <==
|------------|-----------------|----------|------------|---------|---------------------|---------------------|
|  Command   |      Args       | Profile  |    User    | Version |     Start Time      |      End Time       |
|------------|-----------------|----------|------------|---------|---------------------|---------------------|
| start      |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:23 -05 |                     |
| kubectl    | -- get po -A    | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:24 -05 |                     |
| start      |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:24 -05 |                     |
| start      | --driver=docker | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:28 -05 |                     |
| delete     |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:29 -05 | 17 Feb 25 18:29 -05 |
| start      | --driver=docker | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:29 -05 |                     |
| delete     |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:32 -05 | 17 Feb 25 18:32 -05 |
| start      | --driver=docker | minikube | LORE\karen | v1.35.0 | 17 Feb 25 18:47 -05 | 17 Feb 25 18:49 -05 |
| docker-env |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 19:58 -05 | 17 Feb 25 19:58 -05 |
| docker-env |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 19:59 -05 | 17 Feb 25 19:59 -05 |
| ip         |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 20:15 -05 | 17 Feb 25 20:15 -05 |
| service    | nodejs-app      | minikube | LORE\karen | v1.35.0 | 17 Feb 25 20:24 -05 | 17 Feb 25 20:25 -05 |
| service    | nodejs-app      | minikube | LORE\karen | v1.35.0 | 17 Feb 25 20:26 -05 | 17 Feb 25 20:30 -05 |
| stop       |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 20:44 -05 | 17 Feb 25 20:45 -05 |
| start      |                 | minikube | LORE\karen | v1.35.0 | 17 Feb 25 20:45 -05 | 17 Feb 25 20:46 -05 |
|------------|-----------------|----------|------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/17 20:45:14
Running on machine: Lore
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0217 20:45:14.890798   25604 out.go:345] Setting OutFile to fd 112 ...
I0217 20:45:14.917724   25604 out.go:397] isatty.IsTerminal(112) = true
I0217 20:45:14.917724   25604 out.go:358] Setting ErrFile to fd 116...
I0217 20:45:14.918502   25604 out.go:397] isatty.IsTerminal(116) = true
I0217 20:45:14.947840   25604 out.go:352] Setting JSON to false
I0217 20:45:14.957157   25604 start.go:129] hostinfo: {"hostname":"Lore","uptime":372449,"bootTime":1739470665,"procs":320,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3194 Build 26100.3194","kernelVersion":"10.0.26100.3194 Build 26100.3194","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d7cecdfc-fddc-4740-83d4-eab1bb38e0c4"}
W0217 20:45:14.957157   25604 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0217 20:45:14.960065   25604 out.go:177] 😄  minikube v1.35.0 en Microsoft Windows 11 Home Single Language 10.0.26100.3194 Build 26100.3194
I0217 20:45:14.963281   25604 notify.go:220] Checking for updates...
I0217 20:45:14.964442   25604 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0217 20:45:14.965598   25604 driver.go:394] Setting default libvirt URI to qemu:///system
I0217 20:45:15.112833   25604 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0217 20:45:15.121888   25604 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0217 20:45:18.439173   25604 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.3172847s)
I0217 20:45:18.443129   25604 info.go:266] docker info: {ID:e42defb3-6460-4322-9a3a-d68549199a89 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:61 OomKillDisable:true NGoroutines:92 SystemTime:2025-02-18 01:45:18.361701427 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3986710528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0217 20:45:18.446813   25604 out.go:177] ✨  Using the docker driver based on existing profile
I0217 20:45:18.451895   25604 start.go:297] selected driver: docker
I0217 20:45:18.451895   25604 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\karen:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0217 20:45:18.451895   25604 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0217 20:45:18.472790   25604 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0217 20:45:18.785078   25604 info.go:266] docker info: {ID:e42defb3-6460-4322-9a3a-d68549199a89 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:61 OomKillDisable:true NGoroutines:92 SystemTime:2025-02-18 01:45:18.764213411 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:16 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3986710528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0217 20:45:18.904555   25604 cni.go:84] Creating CNI manager for ""
I0217 20:45:18.904555   25604 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0217 20:45:18.905121   25604 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\karen:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0217 20:45:18.907382   25604 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0217 20:45:18.913084   25604 cache.go:121] Beginning downloading kic base image for docker with docker
I0217 20:45:18.916812   25604 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0217 20:45:18.921510   25604 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0217 20:45:18.921510   25604 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0217 20:45:18.923084   25604 preload.go:146] Found local preload: C:\Users\karen\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0217 20:45:18.923084   25604 cache.go:56] Caching tarball of preloaded images
I0217 20:45:18.923084   25604 preload.go:172] Found C:\Users\karen\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0217 20:45:18.923084   25604 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0217 20:45:18.923084   25604 profile.go:143] Saving config to C:\Users\karen\.minikube\profiles\minikube\config.json ...
I0217 20:45:19.077851   25604 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0217 20:45:19.078933   25604 localpath.go:146] windows sanitize: C:\Users\karen\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\karen\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0217 20:45:19.078933   25604 localpath.go:146] windows sanitize: C:\Users\karen\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\karen\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0217 20:45:19.078933   25604 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0217 20:45:19.080021   25604 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0217 20:45:19.080021   25604 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0217 20:45:19.080021   25604 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0217 20:45:19.080021   25604 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0217 20:45:19.080021   25604 localpath.go:146] windows sanitize: C:\Users\karen\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\karen\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0217 20:45:49.334133   25604 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0217 20:45:49.334683   25604 cache.go:227] Successfully downloaded all kic artifacts
I0217 20:45:49.337955   25604 start.go:360] acquireMachinesLock for minikube: {Name:mke93d924f65d86a1b616828a6c44bfe8211837e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0217 20:45:49.338553   25604 start.go:364] duration metric: took 597.4µs to acquireMachinesLock for "minikube"
I0217 20:45:49.338553   25604 start.go:96] Skipping create...Using existing machine configuration
I0217 20:45:49.340167   25604 fix.go:54] fixHost starting: 
I0217 20:45:49.384567   25604 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 20:45:49.457849   25604 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0217 20:45:49.457849   25604 fix.go:138] unexpected machine state, will restart: <nil>
I0217 20:45:49.462282   25604 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0217 20:45:49.488664   25604 cli_runner.go:164] Run: docker start minikube
I0217 20:45:51.370194   25604 cli_runner.go:217] Completed: docker start minikube: (1.8815301s)
I0217 20:45:51.391347   25604 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 20:45:51.473541   25604 kic.go:430] container "minikube" state is running.
I0217 20:45:51.489951   25604 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 20:45:51.591982   25604 profile.go:143] Saving config to C:\Users\karen\.minikube\profiles\minikube\config.json ...
I0217 20:45:51.595590   25604 machine.go:93] provisionDockerMachine start ...
I0217 20:45:51.609052   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:51.685791   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:51.689128   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:51.689128   25604 main.go:141] libmachine: About to run SSH command:
hostname
I0217 20:45:51.693487   25604 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0217 20:45:54.900388   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0217 20:45:54.901555   25604 ubuntu.go:169] provisioning hostname "minikube"
I0217 20:45:54.924908   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:55.023891   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:55.024426   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:55.024426   25604 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0217 20:45:55.213199   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0217 20:45:55.225555   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:55.301903   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:55.302447   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:55.302447   25604 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0217 20:45:55.471892   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0217 20:45:55.471892   25604 ubuntu.go:175] set auth options {CertDir:C:\Users\karen\.minikube CaCertPath:C:\Users\karen\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\karen\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\karen\.minikube\machines\server.pem ServerKeyPath:C:\Users\karen\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\karen\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\karen\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\karen\.minikube}
I0217 20:45:55.471892   25604 ubuntu.go:177] setting up certificates
I0217 20:45:55.471892   25604 provision.go:84] configureAuth start
I0217 20:45:55.487606   25604 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 20:45:55.560254   25604 provision.go:143] copyHostCerts
I0217 20:45:55.562411   25604 exec_runner.go:144] found C:\Users\karen\.minikube/ca.pem, removing ...
I0217 20:45:55.562948   25604 exec_runner.go:203] rm: C:\Users\karen\.minikube\ca.pem
I0217 20:45:55.563483   25604 exec_runner.go:151] cp: C:\Users\karen\.minikube\certs\ca.pem --> C:\Users\karen\.minikube/ca.pem (1074 bytes)
I0217 20:45:55.565095   25604 exec_runner.go:144] found C:\Users\karen\.minikube/cert.pem, removing ...
I0217 20:45:55.565095   25604 exec_runner.go:203] rm: C:\Users\karen\.minikube\cert.pem
I0217 20:45:55.565438   25604 exec_runner.go:151] cp: C:\Users\karen\.minikube\certs\cert.pem --> C:\Users\karen\.minikube/cert.pem (1119 bytes)
I0217 20:45:55.567451   25604 exec_runner.go:144] found C:\Users\karen\.minikube/key.pem, removing ...
I0217 20:45:55.567451   25604 exec_runner.go:203] rm: C:\Users\karen\.minikube\key.pem
I0217 20:45:55.567451   25604 exec_runner.go:151] cp: C:\Users\karen\.minikube\certs\key.pem --> C:\Users\karen\.minikube/key.pem (1675 bytes)
I0217 20:45:55.568534   25604 provision.go:117] generating server cert: C:\Users\karen\.minikube\machines\server.pem ca-key=C:\Users\karen\.minikube\certs\ca.pem private-key=C:\Users\karen\.minikube\certs\ca-key.pem org=karen.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0217 20:45:55.816214   25604 provision.go:177] copyRemoteCerts
I0217 20:45:55.818890   25604 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0217 20:45:55.835322   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:55.922135   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:45:56.056721   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0217 20:45:56.126668   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0217 20:45:56.180182   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0217 20:45:56.210951   25604 provision.go:87] duration metric: took 739.0591ms to configureAuth
I0217 20:45:56.210951   25604 ubuntu.go:193] setting minikube options for container-runtime
I0217 20:45:56.211514   25604 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0217 20:45:56.220528   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:56.296495   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:56.297045   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:56.297045   25604 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0217 20:45:56.462383   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0217 20:45:56.462383   25604 ubuntu.go:71] root file system type: overlay
I0217 20:45:56.462383   25604 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0217 20:45:56.483620   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:56.585347   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:56.585886   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:56.585886   25604 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0217 20:45:56.778543   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0217 20:45:56.800144   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:56.886223   25604 main.go:141] libmachine: Using SSH client type: native
I0217 20:45:56.886792   25604 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe05360] 0xe07ea0 <nil>  [] 0s} 127.0.0.1 55744 <nil> <nil>}
I0217 20:45:56.886792   25604 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0217 20:45:57.080208   25604 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0217 20:45:57.080208   25604 machine.go:96] duration metric: took 5.4846181s to provisionDockerMachine
I0217 20:45:57.080723   25604 start.go:293] postStartSetup for "minikube" (driver="docker")
I0217 20:45:57.080723   25604 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0217 20:45:57.083853   25604 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0217 20:45:57.097857   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:57.179887   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:45:57.324139   25604 ssh_runner.go:195] Run: cat /etc/os-release
I0217 20:45:57.331831   25604 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0217 20:45:57.331831   25604 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0217 20:45:57.331831   25604 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0217 20:45:57.331831   25604 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0217 20:45:57.331831   25604 filesync.go:126] Scanning C:\Users\karen\.minikube\addons for local assets ...
I0217 20:45:57.332435   25604 filesync.go:126] Scanning C:\Users\karen\.minikube\files for local assets ...
I0217 20:45:57.332966   25604 start.go:296] duration metric: took 252.2439ms for postStartSetup
I0217 20:45:57.350903   25604 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0217 20:45:57.362637   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:57.433021   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:45:57.564002   25604 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0217 20:45:57.573643   25604 fix.go:56] duration metric: took 8.2340111s for fixHost
I0217 20:45:57.573643   25604 start.go:83] releasing machines lock for "minikube", held for 8.2350902s
I0217 20:45:57.587791   25604 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0217 20:45:57.813788   25604 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0217 20:45:57.829304   25604 ssh_runner.go:195] Run: cat /version.json
I0217 20:45:57.847354   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:57.880822   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:45:57.985700   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:45:58.184880   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:45:58.328465   25604 ssh_runner.go:195] Run: systemctl --version
I0217 20:45:58.459041   25604 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0217 20:45:58.502833   25604 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0217 20:45:58.570278   25604 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
W0217 20:45:58.695921   25604 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0217 20:45:58.700694   25604 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0217 20:45:58.741642   25604 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0217 20:45:58.741642   25604 start.go:495] detecting cgroup driver to use...
I0217 20:45:58.741642   25604 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0217 20:45:59.061352   25604 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0217 20:45:59.156372   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0217 20:45:59.218193   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0217 20:45:59.233235   25604 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0217 20:45:59.243534   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0217 20:45:59.268256   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0217 20:45:59.297629   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0217 20:45:59.321834   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0217 20:45:59.347883   25604 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0217 20:45:59.371313   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0217 20:45:59.396168   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0217 20:45:59.424699   25604 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0217 20:45:59.439907   25604 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0217 20:45:59.455379   25604 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0217 20:45:59.468228   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0217 20:45:59.527879   25604 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0217 20:45:59.528393   25604 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0217 20:45:59.600878   25604 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0217 20:45:59.809185   25604 start.go:495] detecting cgroup driver to use...
I0217 20:45:59.809185   25604 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0217 20:45:59.812757   25604 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0217 20:45:59.830950   25604 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0217 20:45:59.832853   25604 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0217 20:45:59.849859   25604 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0217 20:45:59.883951   25604 ssh_runner.go:195] Run: which cri-dockerd
I0217 20:45:59.890830   25604 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0217 20:45:59.900650   25604 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0217 20:45:59.922178   25604 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0217 20:46:00.042354   25604 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0217 20:46:00.189632   25604 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0217 20:46:00.190391   25604 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0217 20:46:00.234903   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 20:46:00.380265   25604 ssh_runner.go:195] Run: sudo systemctl restart docker
I0217 20:46:04.538216   25604 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.1579511s)
I0217 20:46:04.540133   25604 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0217 20:46:04.557126   25604 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0217 20:46:04.584414   25604 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0217 20:46:04.598972   25604 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0217 20:46:04.698620   25604 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0217 20:46:04.808369   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 20:46:04.921846   25604 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0217 20:46:04.945526   25604 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0217 20:46:04.964791   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 20:46:05.095451   25604 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0217 20:46:05.886966   25604 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0217 20:46:05.912557   25604 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0217 20:46:05.922531   25604 start.go:563] Will wait 60s for crictl version
I0217 20:46:05.942878   25604 ssh_runner.go:195] Run: which crictl
I0217 20:46:05.954512   25604 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0217 20:46:06.324811   25604 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0217 20:46:06.333195   25604 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0217 20:46:06.400681   25604 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0217 20:46:06.443809   25604 out.go:235] 🐳  Preparando Kubernetes v1.32.0 en Docker 27.4.1...
I0217 20:46:06.455005   25604 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0217 20:46:06.880516   25604 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0217 20:46:06.897603   25604 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0217 20:46:06.905032   25604 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0217 20:46:06.937248   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 20:46:07.019168   25604 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\karen:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0217 20:46:07.019721   25604 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0217 20:46:07.032680   25604 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0217 20:46:07.064544   25604 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
devopshint/node-app:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0217 20:46:07.064544   25604 docker.go:619] Images already preloaded, skipping extraction
I0217 20:46:07.081649   25604 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0217 20:46:07.112273   25604 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
devopshint/node-app:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0217 20:46:07.112273   25604 cache_images.go:84] Images are preloaded, skipping loading
I0217 20:46:07.112273   25604 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0217 20:46:07.113894   25604 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0217 20:46:07.126259   25604 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0217 20:46:07.526575   25604 cni.go:84] Creating CNI manager for ""
I0217 20:46:07.526575   25604 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0217 20:46:07.526575   25604 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0217 20:46:07.526575   25604 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0217 20:46:07.527105   25604 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0217 20:46:07.528702   25604 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0217 20:46:07.545603   25604 binaries.go:44] Found k8s binaries, skipping transfer
I0217 20:46:07.548162   25604 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0217 20:46:07.563034   25604 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0217 20:46:07.584474   25604 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0217 20:46:07.607169   25604 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0217 20:46:07.648376   25604 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0217 20:46:07.657768   25604 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0217 20:46:07.696801   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 20:46:07.899465   25604 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0217 20:46:08.154159   25604 certs.go:68] Setting up C:\Users\karen\.minikube\profiles\minikube for IP: 192.168.49.2
I0217 20:46:08.154159   25604 certs.go:194] generating shared ca certs ...
I0217 20:46:08.154159   25604 certs.go:226] acquiring lock for ca certs: {Name:mk978f9d51a4d72302e41f9d3e8a79a29170ad42 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 20:46:08.416996   25604 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\karen\.minikube\ca.key
I0217 20:46:08.557510   25604 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\karen\.minikube\proxy-client-ca.key
I0217 20:46:08.558018   25604 certs.go:256] generating profile certs ...
I0217 20:46:08.560117   25604 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\karen\.minikube\profiles\minikube\client.key
I0217 20:46:08.602709   25604 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\karen\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0217 20:46:08.648824   25604 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\karen\.minikube\profiles\minikube\proxy-client.key
I0217 20:46:08.656104   25604 certs.go:484] found cert: C:\Users\karen\.minikube\certs\ca-key.pem (1679 bytes)
I0217 20:46:08.656104   25604 certs.go:484] found cert: C:\Users\karen\.minikube\certs\ca.pem (1074 bytes)
I0217 20:46:08.656894   25604 certs.go:484] found cert: C:\Users\karen\.minikube\certs\cert.pem (1119 bytes)
I0217 20:46:08.656894   25604 certs.go:484] found cert: C:\Users\karen\.minikube\certs\key.pem (1675 bytes)
I0217 20:46:08.662932   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0217 20:46:08.701002   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0217 20:46:08.736486   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0217 20:46:08.768952   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0217 20:46:08.801266   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0217 20:46:08.831151   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0217 20:46:08.857741   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0217 20:46:08.884521   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0217 20:46:08.916051   25604 ssh_runner.go:362] scp C:\Users\karen\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0217 20:46:09.005243   25604 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0217 20:46:09.072258   25604 ssh_runner.go:195] Run: openssl version
I0217 20:46:09.098187   25604 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0217 20:46:09.130063   25604 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0217 20:46:09.139039   25604 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 17 23:49 /usr/share/ca-certificates/minikubeCA.pem
I0217 20:46:09.157430   25604 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0217 20:46:09.180973   25604 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0217 20:46:09.207211   25604 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0217 20:46:09.230876   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0217 20:46:09.259659   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0217 20:46:09.287943   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0217 20:46:09.310198   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0217 20:46:09.340350   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0217 20:46:09.372156   25604 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0217 20:46:09.385749   25604 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\karen:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0217 20:46:09.397337   25604 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0217 20:46:09.446485   25604 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0217 20:46:09.458332   25604 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0217 20:46:09.460610   25604 kubeadm.go:593] restartPrimaryControlPlane start ...
I0217 20:46:09.464492   25604 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0217 20:46:09.478904   25604 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0217 20:46:09.497012   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 20:46:09.591646   25604 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\karen\.kube\config
I0217 20:46:09.591968   25604 kubeconfig.go:62] C:\Users\karen\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0217 20:46:09.594163   25604 lock.go:35] WriteFile acquiring C:\Users\karen\.kube\config: {Name:mk00c51cb1fb3534565f28e411195122c4d6d86b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 20:46:09.662214   25604 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0217 20:46:09.724842   25604 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0217 20:46:09.724842   25604 kubeadm.go:597] duration metric: took 264.2313ms to restartPrimaryControlPlane
I0217 20:46:09.724842   25604 kubeadm.go:394] duration metric: took 339.0926ms to StartCluster
I0217 20:46:09.725687   25604 settings.go:142] acquiring lock: {Name:mkd7cb2bf57403341ea9fda42054626016827148 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 20:46:09.726401   25604 settings.go:150] Updating kubeconfig:  C:\Users\karen\.kube\config
I0217 20:46:09.729258   25604 lock.go:35] WriteFile acquiring C:\Users\karen\.kube\config: {Name:mk00c51cb1fb3534565f28e411195122c4d6d86b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0217 20:46:09.738022   25604 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0217 20:46:09.752517   25604 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0217 20:46:09.757821   25604 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0217 20:46:09.757821   25604 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0217 20:46:09.757821   25604 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0217 20:46:09.759581   25604 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0217 20:46:09.759581   25604 addons.go:247] addon storage-provisioner should already be in state true
I0217 20:46:09.761179   25604 out.go:177] 🔎  Verifying Kubernetes components...
I0217 20:46:09.761179   25604 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0217 20:46:09.761179   25604 host.go:66] Checking if "minikube" exists ...
I0217 20:46:09.766200   25604 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0217 20:46:09.795055   25604 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 20:46:09.797255   25604 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 20:46:09.887525   25604 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0217 20:46:09.890637   25604 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0217 20:46:09.890637   25604 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0217 20:46:09.904138   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:46:09.905371   25604 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0217 20:46:09.905371   25604 addons.go:247] addon default-storageclass should already be in state true
I0217 20:46:09.905896   25604 host.go:66] Checking if "minikube" exists ...
I0217 20:46:09.951114   25604 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0217 20:46:10.002968   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:46:10.056469   25604 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0217 20:46:10.056469   25604 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0217 20:46:10.066893   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0217 20:46:10.147648   25604 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0217 20:46:10.163200   25604 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:55744 SSHKeyPath:C:\Users\karen\.minikube\machines\minikube\id_rsa Username:docker}
I0217 20:46:10.243893   25604 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0217 20:46:10.256245   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0217 20:46:10.314953   25604 api_server.go:52] waiting for apiserver process to appear ...
I0217 20:46:10.316577   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 20:46:10.347262   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0217 20:46:10.940036   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:10.940036   25604 retry.go:31] will retry after 206.862464ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:10.942835   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 20:46:11.020602   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.020602   25604 retry.go:31] will retry after 349.19846ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.148922   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0217 20:46:11.265133   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.265133   25604 retry.go:31] will retry after 417.770918ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.319872   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 20:46:11.372202   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0217 20:46:11.686017   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0217 20:46:11.736304   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.736304   25604 retry.go:31] will retry after 264.284695ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:11.823058   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 20:46:12.003572   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0217 20:46:12.242064   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:12.242064   25604 retry.go:31] will retry after 539.696695ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:12.323886   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 20:46:12.635062   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:12.635062   25604 retry.go:31] will retry after 501.762833ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:12.786995   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0217 20:46:12.818121   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 20:46:12.938861   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:12.938861   25604 retry.go:31] will retry after 1.085818612s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:13.141256   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0217 20:46:13.320732   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0217 20:46:13.635724   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:13.635724   25604 retry.go:31] will retry after 547.448717ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:13.827320   25604 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0217 20:46:13.924274   25604 api_server.go:72] duration metric: took 4.1680775s to wait for apiserver process to appear ...
I0217 20:46:13.924274   25604 api_server.go:88] waiting for apiserver healthz status ...
I0217 20:46:13.924274   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:13.931065   25604 api_server.go:269] stopped: https://127.0.0.1:55743/healthz: Get "https://127.0.0.1:55743/healthz": EOF
I0217 20:46:14.033209   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0217 20:46:14.187819   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0217 20:46:14.425078   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
W0217 20:46:14.425674   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:14.425674   25604 retry.go:31] will retry after 1.371641694s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:14.432772   25604 api_server.go:269] stopped: https://127.0.0.1:55743/healthz: Get "https://127.0.0.1:55743/healthz": EOF
W0217 20:46:14.629118   25604 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:14.629118   25604 retry.go:31] will retry after 1.77844025s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0217 20:46:14.924774   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:14.927259   25604 api_server.go:269] stopped: https://127.0.0.1:55743/healthz: Get "https://127.0.0.1:55743/healthz": EOF
I0217 20:46:15.424865   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:15.430034   25604 api_server.go:269] stopped: https://127.0.0.1:55743/healthz: Get "https://127.0.0.1:55743/healthz": EOF
I0217 20:46:15.799715   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0217 20:46:15.924961   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:16.409696   25604 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0217 20:46:20.926206   25604 api_server.go:269] stopped: https://127.0.0.1:55743/healthz: Get "https://127.0.0.1:55743/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0217 20:46:20.926206   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:25.035618   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0217 20:46:25.035618   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0217 20:46:25.035618   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:25.219874   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:25.219874   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:25.424713   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:25.439778   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:25.439778   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:25.925144   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:26.012436   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:26.012436   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:26.425009   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:26.524415   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:26.524927   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:26.924973   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:27.017970   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:27.017970   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:27.424496   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:27.435661   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:27.435661   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:27.924650   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:27.941980   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:27.941980   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:28.424976   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:28.437545   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0217 20:46:28.437545   25604 api_server.go:103] status: https://127.0.0.1:55743/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0217 20:46:28.924978   25604 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:55743/healthz ...
I0217 20:46:28.942770   25604 api_server.go:279] https://127.0.0.1:55743/healthz returned 200:
ok
I0217 20:46:29.031233   25604 api_server.go:141] control plane version: v1.32.0
I0217 20:46:29.031233   25604 api_server.go:131] duration metric: took 15.1069585s to wait for apiserver health ...
I0217 20:46:29.032410   25604 system_pods.go:43] waiting for kube-system pods to appear ...
I0217 20:46:29.083503   25604 system_pods.go:59] 7 kube-system pods found
I0217 20:46:29.083503   25604 system_pods.go:61] "coredns-668d6bf9bc-8x4dv" [8ec86c99-8f61-4adc-8dae-82ebd55a53c4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0217 20:46:29.083503   25604 system_pods.go:61] "etcd-minikube" [4e7ee61b-6c4d-4588-8d60-380bf4dd4016] Running
I0217 20:46:29.083503   25604 system_pods.go:61] "kube-apiserver-minikube" [695d4ac0-c4ac-44fd-a40d-b718c263ca93] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0217 20:46:29.083503   25604 system_pods.go:61] "kube-controller-manager-minikube" [528959b4-ef6e-44d7-8fc8-8ba0fd4a622d] Running
I0217 20:46:29.083503   25604 system_pods.go:61] "kube-proxy-prqtc" [71aee8b0-9105-4f6f-9b87-18b6b573a6d5] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0217 20:46:29.083503   25604 system_pods.go:61] "kube-scheduler-minikube" [d60a5c81-4ec9-45c5-a967-9ede53e60296] Running
I0217 20:46:29.083503   25604 system_pods.go:61] "storage-provisioner" [925e848f-1ffe-4bb2-b862-1d319f191ce5] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0217 20:46:29.083503   25604 system_pods.go:74] duration metric: took 51.0927ms to wait for pod list to return data ...
I0217 20:46:29.083503   25604 kubeadm.go:582] duration metric: took 19.327306s to wait for: map[apiserver:true system_pods:true]
I0217 20:46:29.083503   25604 node_conditions.go:102] verifying NodePressure condition ...
I0217 20:46:29.117745   25604 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0217 20:46:29.117745   25604 node_conditions.go:123] node cpu capacity is 8
I0217 20:46:29.118326   25604 node_conditions.go:105] duration metric: took 34.823ms to run NodePressure ...
I0217 20:46:29.118326   25604 start.go:241] waiting for startup goroutines ...
I0217 20:46:31.923303   25604 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (15.513046s)
I0217 20:46:31.923303   25604 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (16.1235877s)
I0217 20:46:32.120078   25604 out.go:177] 🌟  Complementos habilitados: storage-provisioner, default-storageclass
I0217 20:46:32.127882   25604 addons.go:514] duration metric: took 22.3866415s for enable addons: enabled=[storage-provisioner default-storageclass]
I0217 20:46:32.127882   25604 start.go:246] waiting for cluster config update ...
I0217 20:46:32.127882   25604 start.go:255] writing updated cluster config ...
I0217 20:46:32.169814   25604 ssh_runner.go:195] Run: rm -f paused
I0217 20:46:32.396326   25604 start.go:600] kubectl: 1.31.4, cluster: 1.32.0 (minor skew: 1)
I0217 20:46:32.401687   25604 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 18 01:45:59 minikube dockerd[811]: time="2025-02-18T01:45:59.861740705Z" level=info msg="Starting up"
Feb 18 01:45:59 minikube dockerd[811]: time="2025-02-18T01:45:59.864140625Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 18 01:45:59 minikube dockerd[811]: time="2025-02-18T01:45:59.894225475Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 18 01:45:59 minikube dockerd[811]: time="2025-02-18T01:45:59.944128100Z" level=info msg="Loading containers: start."
Feb 18 01:46:00 minikube dockerd[811]: time="2025-02-18T01:46:00.389577476Z" level=info msg="Processing signal 'terminated'"
Feb 18 01:46:01 minikube dockerd[811]: time="2025-02-18T01:46:01.721748230Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.067847715Z" level=info msg="Loading containers: done."
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117339973Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117415863Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117430047Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117437221Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117463296Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.117527262Z" level=info msg="Daemon has completed initialization"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.219637395Z" level=info msg="API listen on /var/run/docker.sock"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.220087046Z" level=info msg="API listen on [::]:2376"
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.221748832Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 18 01:46:02 minikube dockerd[811]: time="2025-02-18T01:46:02.223239833Z" level=info msg="Daemon shutdown complete"
Feb 18 01:46:02 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 18 01:46:02 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 18 01:46:02 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 18 01:46:02 minikube dockerd[1095]: time="2025-02-18T01:46:02.277480840Z" level=info msg="Starting up"
Feb 18 01:46:02 minikube dockerd[1095]: time="2025-02-18T01:46:02.280361283Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 18 01:46:02 minikube dockerd[1095]: time="2025-02-18T01:46:02.311827683Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 18 01:46:02 minikube dockerd[1095]: time="2025-02-18T01:46:02.339613686Z" level=info msg="Loading containers: start."
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.051733700Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.435198913Z" level=info msg="Loading containers: done."
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456279567Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456335123Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456345530Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456350747Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456374506Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.456433584Z" level=info msg="Daemon has completed initialization"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.535979909Z" level=info msg="API listen on [::]:2376"
Feb 18 01:46:04 minikube dockerd[1095]: time="2025-02-18T01:46:04.536002798Z" level=info msg="API listen on /var/run/docker.sock"
Feb 18 01:46:04 minikube systemd[1]: Started Docker Application Container Engine.
Feb 18 01:46:05 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Start docker client with request timeout 0s"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Loaded network plugin cni"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 18 01:46:05 minikube cri-dockerd[1390]: time="2025-02-18T01:46:05Z" level=info msg="Start cri-dockerd grpc backend"
Feb 18 01:46:05 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 18 01:46:10 minikube cri-dockerd[1390]: time="2025-02-18T01:46:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nodejs-app-8c5d666f9-vr8vw_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"545bb090eb7e958ccb44eccad3d0c8ca8d774a2ac51a4b81da55611bc02a83b9\""
Feb 18 01:46:10 minikube cri-dockerd[1390]: time="2025-02-18T01:46:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-8x4dv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"96582800f2f1ba9bf13d3b65a7fa682531a56ad2490da086b253899cf7fd04e3\""
Feb 18 01:46:12 minikube cri-dockerd[1390]: time="2025-02-18T01:46:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c2b01acb237dad18672cba76389209c32b759f90c886e6d57cfb53f0700265e4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:12 minikube cri-dockerd[1390]: time="2025-02-18T01:46:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a002bdeb74fffe1253a0a6252e7e02e73fc96b00037273a666b2484a0fa635c4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:12 minikube cri-dockerd[1390]: time="2025-02-18T01:46:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/91f8485d81df23d0c71a53b70da8c6d529445ca8b8033fde74732e93807b11de/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:12 minikube cri-dockerd[1390]: time="2025-02-18T01:46:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47f8be19512cc60f8e3f0d3077a86ffceedf4bcbc83d7b0f6877320e41a4da19/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:25 minikube cri-dockerd[1390]: time="2025-02-18T01:46:25Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 18 01:46:28 minikube cri-dockerd[1390]: time="2025-02-18T01:46:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7ac192ba8033f067b8e8223925817acda27d4da25b625445d68be69be8b229b7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:29 minikube cri-dockerd[1390]: time="2025-02-18T01:46:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bee0d13c8aac5163ab57d743991ee81ca934dfbe411d9f2eb344f0c2d26578a4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:32 minikube cri-dockerd[1390]: time="2025-02-18T01:46:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c336f988a6a25e02d98903dda3ed3753862346e1fca4f74830c1b89dc26a69c5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 18 01:46:53 minikube dockerd[1095]: time="2025-02-18T01:46:53.698659123Z" level=info msg="ignoring event" container=dac13763c2f5baee092a9ad2dd7051badc4369ec34cffb5debeb7908998ed3d5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 18 02:04:39 minikube cri-dockerd[1390]: time="2025-02-18T02:04:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b94701297c34972030445247fc156d6f574d59067b605a4651f4a9c49e1b016/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 18 02:04:40 minikube cri-dockerd[1390]: time="2025-02-18T02:04:40Z" level=info msg="Stop pulling image devopshint/node-app:latest: Status: Image is up to date for devopshint/node-app:latest"


==> container status <==
CONTAINER           IMAGE                                                                                         CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
14b847f1e62a8       devopshint/node-app@sha256:07efd4763fa7b96ca6227e88a126e3e9622708dde42f9afef1d31e166eec2c65   About a minute ago   Running             nodejs-app                0                   9b94701297c34       nodejs-app-8c5d666f9-2w4rk
d6f4f17edfd6a       6e38f40d628db                                                                                 18 minutes ago       Running             storage-provisioner       3                   bee0d13c8aac5       storage-provisioner
a8c432d190dbd       c69fa2e9cbf5f                                                                                 19 minutes ago       Running             coredns                   1                   c336f988a6a25       coredns-668d6bf9bc-8x4dv
dac13763c2f5b       6e38f40d628db                                                                                 19 minutes ago       Exited              storage-provisioner       2                   bee0d13c8aac5       storage-provisioner
c58260b7eed3d       040f9f8aac8cd                                                                                 19 minutes ago       Running             kube-proxy                1                   7ac192ba8033f       kube-proxy-prqtc
e26edca000cab       8cab3d2a8bd0f                                                                                 19 minutes ago       Running             kube-controller-manager   1                   a002bdeb74fff       kube-controller-manager-minikube
d6476896931e7       a9e7e6b294baf                                                                                 19 minutes ago       Running             etcd                      1                   91f8485d81df2       etcd-minikube
05158569c42e4       c2e17b8d0f4a3                                                                                 19 minutes ago       Running             kube-apiserver            1                   c2b01acb237da       kube-apiserver-minikube
9d2f8fe6fea5f       a389e107f4ff1                                                                                 19 minutes ago       Running             kube-scheduler            1                   47f8be19512cc       kube-scheduler-minikube
ced6708e9e193       c69fa2e9cbf5f                                                                                 2 hours ago          Exited              coredns                   0                   96582800f2f1b       coredns-668d6bf9bc-8x4dv
256647b30d91f       040f9f8aac8cd                                                                                 2 hours ago          Exited              kube-proxy                0                   dcae02eaef65f       kube-proxy-prqtc
a23503d0bd155       c2e17b8d0f4a3                                                                                 2 hours ago          Exited              kube-apiserver            0                   5107678c8586a       kube-apiserver-minikube
9b102c1810d13       a389e107f4ff1                                                                                 2 hours ago          Exited              kube-scheduler            0                   af7b278f654a7       kube-scheduler-minikube
2bf6b7221ccce       a9e7e6b294baf                                                                                 2 hours ago          Exited              etcd                      0                   45d19852f3dfd       etcd-minikube
aa955594e5602       8cab3d2a8bd0f                                                                                 2 hours ago          Exited              kube-controller-manager   0                   20e3192d8b80a       kube-controller-manager-minikube


==> coredns [a8c432d190db] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:56942 - 17059 "HINFO IN 4599518838257145621.7060529073730438776. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.078169718s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[6694279]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Feb-2025 01:46:33.984) (total time: 21077ms):
Trace[6694279]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21073ms (01:46:55.054)
Trace[6694279]: [21.077585465s] [21.077585465s] END
[INFO] plugin/kubernetes: Trace[634360846]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Feb-2025 01:46:33.984) (total time: 21076ms):
Trace[634360846]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21073ms (01:46:55.054)
Trace[634360846]: [21.076585452s] [21.076585452s] END
[INFO] plugin/kubernetes: Trace[560728109]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Feb-2025 01:46:33.984) (total time: 21076ms):
Trace[560728109]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21074ms (01:46:55.056)
Trace[560728109]: [21.076624852s] [21.076624852s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [ced6708e9e19] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:52350 - 56995 "HINFO IN 5221452546555399816.4003004270065376007. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.597660156s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[680201018]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Feb-2025 23:49:53.630) (total time: 21062ms):
Trace[680201018]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21061ms (23:50:14.690)
Trace[680201018]: [21.062481505s] [21.062481505s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[297784787]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Feb-2025 23:49:53.630) (total time: 21063ms):
Trace[297784787]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21062ms (23:50:14.691)
Trace[297784787]: [21.063026822s] [21.063026822s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[281537835]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (17-Feb-2025 23:49:53.630) (total time: 21062ms):
Trace[281537835]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21062ms (23:50:14.691)
Trace[281537835]: [21.062882666s] [21.062882666s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_17T18_49_46_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 17 Feb 2025 23:49:41 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Feb 2025 02:06:03 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 18 Feb 2025 02:03:28 +0000   Mon, 17 Feb 2025 23:49:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 18 Feb 2025 02:03:28 +0000   Mon, 17 Feb 2025 23:49:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 18 Feb 2025 02:03:28 +0000   Mon, 17 Feb 2025 23:49:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 18 Feb 2025 02:03:28 +0000   Mon, 17 Feb 2025 23:49:42 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3893272Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3893272Ki
  pods:               110
System Info:
  Machine ID:                 8d7e409a1bc1487bb4ac620f4d3399d5
  System UUID:                8d7e409a1bc1487bb4ac620f4d3399d5
  Boot ID:                    27641bb8-5c9c-4903-946d-74f1eee93d77
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nodejs-app-8c5d666f9-2w4rk          0 (0%)        0 (0%)      0 (0%)           0 (0%)         91s
  kube-system                 coredns-668d6bf9bc-8x4dv            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     136m
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         136m
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         136m
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         136m
  kube-system                 kube-proxy-prqtc                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         136m
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         136m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         136m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           19m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  19m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           19m                kubelet          Starting kubelet.
  Warning  CgroupV1                           19m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               19m (x7 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            19m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     19m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Feb18 00:13] PCI: Fatal: No config space access function found
[  +0.019783] PCI: System does not support PCI
[  +0.138007] kvm: already loaded the other module
[  +1.239450] FS-Cache: Duplicate cookie detected
[  +0.000415] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000495] FS-Cache: O-cookie d=000000005b3b4ee0{9P.session} n=00000000638a37c6
[  +0.000301] FS-Cache: O-key=[10] '34323934393337343337'
[  +0.000301] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000731] FS-Cache: N-cookie d=000000005b3b4ee0{9P.session} n=00000000b77988a6
[  +0.000511] FS-Cache: N-key=[10] '34323934393337343337'
[  +0.149577] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#447 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.010300] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#511 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.019969] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#7 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.009506] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#127 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +1.433122] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.008167] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Bogota not found. Is the tzdata package installed?
[  +0.283636] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#7 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.031134] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007563] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000694] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000534] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000676] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.222611] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002010] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000491] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000391] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000505] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.821816] WSL (148) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb18 00:14] netlink: 'init': attribute type 4 has an invalid length.
[Feb18 00:18] tmpfs: Unknown parameter 'noswap'
[ +13.198849] tmpfs: Unknown parameter 'noswap'
[Feb18 00:55] WSL (148) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb18 01:40] WSL (148) ERROR: CheckConnection: getaddrinfo() failed: -5
[Feb18 01:46] tmpfs: Unknown parameter 'noswap'


==> etcd [2bf6b7221ccc] <==
{"level":"warn","ts":"2025-02-18T01:22:33.759733Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"628.385366ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:22:33.763371Z","caller":"traceutil/trace.go:171","msg":"trace[60598866] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3637; }","duration":"671.588216ms","start":"2025-02-18T01:22:33.088259Z","end":"2025-02-18T01:22:33.759847Z","steps":["trace[60598866] 'agreement among raft nodes before linearized reading'  (duration: 628.272482ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:22:33.763278Z","caller":"traceutil/trace.go:171","msg":"trace[568019228] linearizableReadLoop","detail":"{readStateIndex:4464; appliedIndex:4463; }","duration":"627.979539ms","start":"2025-02-18T01:22:33.088319Z","end":"2025-02-18T01:22:33.716299Z","steps":["trace[568019228] 'read index received'  (duration: 47.136µs)","trace[568019228] 'applied index is now lower than readState.Index'  (duration: 627.929338ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:22:33.771868Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"637.576095ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:22:33.772093Z","caller":"traceutil/trace.go:171","msg":"trace[1098556469] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3637; }","duration":"637.919741ms","start":"2025-02-18T01:22:33.134151Z","end":"2025-02-18T01:22:33.772070Z","steps":["trace[1098556469] 'agreement among raft nodes before linearized reading'  (duration: 637.538085ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:22:33.775284Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T01:22:33.088235Z","time spent":"683.145613ms","remote":"127.0.0.1:50742","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-02-18T01:22:54.273358Z","caller":"traceutil/trace.go:171","msg":"trace[1416943563] transaction","detail":"{read_only:false; response_revision:3654; number_of_response:1; }","duration":"109.87296ms","start":"2025-02-18T01:22:54.163443Z","end":"2025-02-18T01:22:54.273316Z","steps":["trace[1416943563] 'process raft request'  (duration: 108.715561ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:22:58.645703Z","caller":"traceutil/trace.go:171","msg":"trace[1361469125] transaction","detail":"{read_only:false; response_revision:3657; number_of_response:1; }","duration":"125.089841ms","start":"2025-02-18T01:22:58.520578Z","end":"2025-02-18T01:22:58.645668Z","steps":["trace[1361469125] 'process raft request'  (duration: 112.455411ms)","trace[1361469125] 'compare'  (duration: 12.350073ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:22:58.965399Z","caller":"traceutil/trace.go:171","msg":"trace[35487907] transaction","detail":"{read_only:false; response_revision:3659; number_of_response:1; }","duration":"104.660492ms","start":"2025-02-18T01:22:58.860693Z","end":"2025-02-18T01:22:58.965353Z","steps":["trace[35487907] 'process raft request'  (duration: 48.228423ms)","trace[35487907] 'compare'  (duration: 56.186383ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:23:15.127197Z","caller":"traceutil/trace.go:171","msg":"trace[2010466199] transaction","detail":"{read_only:false; response_revision:3670; number_of_response:1; }","duration":"129.286371ms","start":"2025-02-18T01:23:14.997847Z","end":"2025-02-18T01:23:15.127133Z","steps":["trace[2010466199] 'process raft request'  (duration: 125.697535ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:23:21.492614Z","caller":"traceutil/trace.go:171","msg":"trace[1927872892] transaction","detail":"{read_only:false; response_revision:3676; number_of_response:1; }","duration":"266.317472ms","start":"2025-02-18T01:23:21.226276Z","end":"2025-02-18T01:23:21.492594Z","steps":["trace[1927872892] 'process raft request'  (duration: 266.131389ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:23:42.353416Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.479071ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:23:42.353820Z","caller":"traceutil/trace.go:171","msg":"trace[1073207777] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3692; }","duration":"121.877229ms","start":"2025-02-18T01:23:42.231911Z","end":"2025-02-18T01:23:42.353788Z","steps":["trace[1073207777] 'range keys from in-memory index tree'  (duration: 104.924703ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:23:42.614255Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.915412ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581481897 > lease_revoke:<id:70cc9516513a115d>","response":"size:29"}
{"level":"warn","ts":"2025-02-18T01:24:22.718886Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"149.955527ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581482103 > lease_revoke:<id:70cc9516513a1222>","response":"size:29"}
{"level":"info","ts":"2025-02-18T01:25:41.412650Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3548}
{"level":"info","ts":"2025-02-18T01:25:41.465962Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3548,"took":"41.848526ms","hash":2181223545,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1089536,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-02-18T01:25:41.468068Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2181223545,"revision":3548,"compact-revision":3306}
{"level":"warn","ts":"2025-02-18T01:26:37.397978Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.09067ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:26:37.398513Z","caller":"traceutil/trace.go:171","msg":"trace[316889335] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3830; }","duration":"254.668392ms","start":"2025-02-18T01:26:37.143457Z","end":"2025-02-18T01:26:37.398125Z","steps":["trace[316889335] 'range keys from in-memory index tree'  (duration: 120.793263ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:26:37.555974Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"148.17985ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581482784 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:3823 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128035350581482782 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-18T01:26:37.556166Z","caller":"traceutil/trace.go:171","msg":"trace[1106680992] transaction","detail":"{read_only:false; response_revision:3831; number_of_response:1; }","duration":"232.819313ms","start":"2025-02-18T01:26:37.323322Z","end":"2025-02-18T01:26:37.556141Z","steps":["trace[1106680992] 'process raft request'  (duration: 57.27508ms)","trace[1106680992] 'compare'  (duration: 147.972993ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:30:41.400484Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3785}
{"level":"info","ts":"2025-02-18T01:30:41.418669Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3785,"took":"16.200524ms","hash":815175665,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":937984,"current-db-size-in-use":"938 kB"}
{"level":"info","ts":"2025-02-18T01:30:41.418843Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":815175665,"revision":3785,"compact-revision":3548}
{"level":"warn","ts":"2025-02-18T01:33:47.422161Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.079088ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581484865 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:4166 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128035350581484863 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-18T01:33:47.422685Z","caller":"traceutil/trace.go:171","msg":"trace[1179604161] transaction","detail":"{read_only:false; response_revision:4174; number_of_response:1; }","duration":"172.4831ms","start":"2025-02-18T01:33:47.249988Z","end":"2025-02-18T01:33:47.422471Z","steps":["trace[1179604161] 'process raft request'  (duration: 59.055955ms)","trace[1179604161] 'compare'  (duration: 110.930837ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:35:04.958891Z","caller":"traceutil/trace.go:171","msg":"trace[679027094] transaction","detail":"{read_only:false; response_revision:4236; number_of_response:1; }","duration":"157.093405ms","start":"2025-02-18T01:35:04.801763Z","end":"2025-02-18T01:35:04.958856Z","steps":["trace[679027094] 'process raft request'  (duration: 156.927158ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:35:41.384478Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4026}
{"level":"info","ts":"2025-02-18T01:35:41.394312Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4026,"took":"9.013736ms","hash":3492909625,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":942080,"current-db-size-in-use":"942 kB"}
{"level":"info","ts":"2025-02-18T01:35:41.394400Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3492909625,"revision":4026,"compact-revision":3785}
{"level":"info","ts":"2025-02-18T01:37:57.491652Z","caller":"traceutil/trace.go:171","msg":"trace[1378475458] transaction","detail":"{read_only:false; response_revision:4373; number_of_response:1; }","duration":"116.200676ms","start":"2025-02-18T01:37:57.375428Z","end":"2025-02-18T01:37:57.491629Z","steps":["trace[1378475458] 'process raft request'  (duration: 116.052695ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:37:57.496976Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.425133ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2025-02-18T01:37:57.565293Z","caller":"traceutil/trace.go:171","msg":"trace[137715311] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:4372; }","duration":"190.111791ms","start":"2025-02-18T01:37:57.375154Z","end":"2025-02-18T01:37:57.565266Z","steps":["trace[137715311] 'range keys from bolt db'  (duration: 112.501453ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:37:58.057611Z","caller":"traceutil/trace.go:171","msg":"trace[1317396419] transaction","detail":"{read_only:false; response_revision:4375; number_of_response:1; }","duration":"156.641562ms","start":"2025-02-18T01:37:57.881101Z","end":"2025-02-18T01:37:58.037743Z","steps":["trace[1317396419] 'process raft request'  (duration: 77.604953ms)","trace[1317396419] 'compare'  (duration: 61.525508ms)","trace[1317396419] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 14.867439ms)"],"step_count":3}
{"level":"warn","ts":"2025-02-18T01:41:15.765049Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.046643ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581486268 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4404 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-18T01:41:15.766671Z","caller":"traceutil/trace.go:171","msg":"trace[216415801] transaction","detail":"{read_only:false; response_revision:4407; number_of_response:1; }","duration":"239.39031ms","start":"2025-02-18T01:41:15.527250Z","end":"2025-02-18T01:41:15.766640Z","steps":["trace[216415801] 'process raft request'  (duration: 72.758988ms)","trace[216415801] 'compare'  (duration: 128.79618ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:41:15.768011Z","caller":"traceutil/trace.go:171","msg":"trace[1198233854] transaction","detail":"{read_only:false; response_revision:4408; number_of_response:1; }","duration":"216.022408ms","start":"2025-02-18T01:41:15.551960Z","end":"2025-02-18T01:41:15.767983Z","steps":["trace[1198233854] 'process raft request'  (duration: 215.692457ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:42:54.634971Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.23446ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581486760 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:4476 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128035350581486758 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-18T01:42:54.635208Z","caller":"traceutil/trace.go:171","msg":"trace[15090917] transaction","detail":"{read_only:false; response_revision:4484; number_of_response:1; }","duration":"247.101264ms","start":"2025-02-18T01:42:54.388068Z","end":"2025-02-18T01:42:54.635169Z","steps":["trace[15090917] 'process raft request'  (duration: 95.822132ms)","trace[15090917] 'compare'  (duration: 119.143766ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:42:54.836910Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"125.702434ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035350581486763 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4483 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"warn","ts":"2025-02-18T01:42:54.837354Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"118.34123ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2025-02-18T01:42:54.870015Z","caller":"traceutil/trace.go:171","msg":"trace[1449441766] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:4485; }","duration":"118.430062ms","start":"2025-02-18T01:42:54.718994Z","end":"2025-02-18T01:42:54.837424Z","steps":["trace[1449441766] 'agreement among raft nodes before linearized reading'  (duration: 118.231417ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:42:54.837129Z","caller":"traceutil/trace.go:171","msg":"trace[1969035906] transaction","detail":"{read_only:false; response_revision:4485; number_of_response:1; }","duration":"197.475199ms","start":"2025-02-18T01:42:54.639624Z","end":"2025-02-18T01:42:54.837099Z","steps":["trace[1969035906] 'process raft request'  (duration: 71.491396ms)","trace[1969035906] 'compare'  (duration: 125.564708ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:42:54.892573Z","caller":"traceutil/trace.go:171","msg":"trace[596838747] linearizableReadLoop","detail":"{readStateIndex:5528; appliedIndex:5527; }","duration":"118.050823ms","start":"2025-02-18T01:42:54.719002Z","end":"2025-02-18T01:42:54.837052Z","steps":["trace[596838747] 'read index received'  (duration: 56.731µs)","trace[596838747] 'applied index is now lower than readState.Index'  (duration: 117.992682ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:42:55.161045Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.392727ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:42:55.161123Z","caller":"traceutil/trace.go:171","msg":"trace[1338563483] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4485; }","duration":"200.205055ms","start":"2025-02-18T01:42:54.960903Z","end":"2025-02-18T01:42:55.161108Z","steps":["trace[1338563483] 'range keys from in-memory index tree'  (duration: 122.380279ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:43:18.311290Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4265}
{"level":"info","ts":"2025-02-18T01:43:18.326311Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4265,"took":"14.020172ms","hash":4062974460,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":942080,"current-db-size-in-use":"942 kB"}
{"level":"info","ts":"2025-02-18T01:43:18.326542Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4062974460,"revision":4265,"compact-revision":4026}
{"level":"info","ts":"2025-02-18T01:45:00.725003Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-02-18T01:45:00.729811Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-02-18T01:45:00.735652Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-18T01:45:00.818614Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-18T01:45:01.020609Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-18T01:45:01.021031Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-02-18T01:45:01.024660Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-18T01:45:01.138445Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-18T01:45:01.223224Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-18T01:45:01.223292Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [d6476896931e] <==
{"level":"info","ts":"2025-02-18T01:46:59.573942Z","caller":"traceutil/trace.go:171","msg":"trace[437247257] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4700; }","duration":"116.134143ms","start":"2025-02-18T01:46:59.457788Z","end":"2025-02-18T01:46:59.573922Z","steps":["trace[437247257] 'agreement among raft nodes before linearized reading'  (duration: 40.824459ms)","trace[437247257] 'range keys from in-memory index tree'  (duration: 75.192146ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:47:06.512742Z","caller":"traceutil/trace.go:171","msg":"trace[823923450] transaction","detail":"{read_only:false; response_revision:4701; number_of_response:1; }","duration":"163.183375ms","start":"2025-02-18T01:47:06.349538Z","end":"2025-02-18T01:47:06.512721Z","steps":["trace[823923450] 'process raft request'  (duration: 163.027016ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:47:06.694311Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.438136ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035352373055115 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:4697 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-18T01:47:06.694618Z","caller":"traceutil/trace.go:171","msg":"trace[1642257935] transaction","detail":"{read_only:false; response_revision:4702; number_of_response:1; }","duration":"328.627055ms","start":"2025-02-18T01:47:06.365971Z","end":"2025-02-18T01:47:06.694598Z","steps":["trace[1642257935] 'process raft request'  (duration: 203.850919ms)","trace[1642257935] 'compare'  (duration: 124.315999ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:47:06.694735Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T01:47:06.365954Z","time spent":"328.743092ms","remote":"127.0.0.1:48574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:4697 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-02-18T01:47:06.738719Z","caller":"traceutil/trace.go:171","msg":"trace[38245480] linearizableReadLoop","detail":"{readStateIndex:5792; appliedIndex:5789; }","duration":"243.531927ms","start":"2025-02-18T01:47:06.495168Z","end":"2025-02-18T01:47:06.738700Z","steps":["trace[38245480] 'read index received'  (duration: 17.316039ms)","trace[38245480] 'applied index is now lower than readState.Index'  (duration: 226.2151ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:47:06.738919Z","caller":"traceutil/trace.go:171","msg":"trace[1146445562] transaction","detail":"{read_only:false; response_revision:4703; number_of_response:1; }","duration":"288.935078ms","start":"2025-02-18T01:47:06.449889Z","end":"2025-02-18T01:47:06.738824Z","steps":["trace[1146445562] 'process raft request'  (duration: 244.499204ms)","trace[1146445562] 'compare'  (duration: 44.191983ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:47:06.739018Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"243.850299ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:47:06.739077Z","caller":"traceutil/trace.go:171","msg":"trace[317758085] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4703; }","duration":"243.931473ms","start":"2025-02-18T01:47:06.495135Z","end":"2025-02-18T01:47:06.739066Z","steps":["trace[317758085] 'agreement among raft nodes before linearized reading'  (duration: 243.722645ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:47:06.749665Z","caller":"traceutil/trace.go:171","msg":"trace[1200082276] transaction","detail":"{read_only:false; response_revision:4705; number_of_response:1; }","duration":"206.825419ms","start":"2025-02-18T01:47:06.542820Z","end":"2025-02-18T01:47:06.749646Z","steps":["trace[1200082276] 'process raft request'  (duration: 206.738264ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:47:06.749714Z","caller":"traceutil/trace.go:171","msg":"trace[341001751] transaction","detail":"{read_only:false; response_revision:4704; number_of_response:1; }","duration":"207.03786ms","start":"2025-02-18T01:47:06.542659Z","end":"2025-02-18T01:47:06.749697Z","steps":["trace[341001751] 'process raft request'  (duration: 206.764867ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:47:07.003189Z","caller":"traceutil/trace.go:171","msg":"trace[432223987] transaction","detail":"{read_only:false; response_revision:4706; number_of_response:1; }","duration":"193.91199ms","start":"2025-02-18T01:47:06.809253Z","end":"2025-02-18T01:47:07.003165Z","steps":["trace[432223987] 'process raft request'  (duration: 193.714816ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:47:07.666191Z","caller":"traceutil/trace.go:171","msg":"trace[221633852] transaction","detail":"{read_only:false; response_revision:4707; number_of_response:1; }","duration":"216.128614ms","start":"2025-02-18T01:47:07.449997Z","end":"2025-02-18T01:47:07.666125Z","steps":["trace[221633852] 'process raft request'  (duration: 215.684948ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:48:47.478116Z","caller":"traceutil/trace.go:171","msg":"trace[1143032303] transaction","detail":"{read_only:false; response_revision:4782; number_of_response:1; }","duration":"124.962272ms","start":"2025-02-18T01:48:47.351042Z","end":"2025-02-18T01:48:47.476004Z","steps":["trace[1143032303] 'process raft request'  (duration: 124.320584ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:49:24.488606Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.421006ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035352373055783 > lease_revoke:<id:70cc9516bc0350d9>","response":"size:29"}
{"level":"info","ts":"2025-02-18T01:55:38.120543Z","caller":"traceutil/trace.go:171","msg":"trace[1772086704] transaction","detail":"{read_only:false; response_revision:5108; number_of_response:1; }","duration":"107.368787ms","start":"2025-02-18T01:55:37.795470Z","end":"2025-02-18T01:55:37.902838Z","steps":["trace[1772086704] 'process raft request'  (duration: 107.246765ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:55:38.820538Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"351.459154ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035352373057590 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:5101 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"warn","ts":"2025-02-18T01:55:38.851850Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"295.285426ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:55:38.852124Z","caller":"traceutil/trace.go:171","msg":"trace[1244403333] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5109; }","duration":"295.450563ms","start":"2025-02-18T01:55:38.556519Z","end":"2025-02-18T01:55:38.851969Z","steps":["trace[1244403333] 'agreement among raft nodes before linearized reading'  (duration: 264.77687ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:55:38.821215Z","caller":"traceutil/trace.go:171","msg":"trace[589542323] transaction","detail":"{read_only:false; response_revision:5109; number_of_response:1; }","duration":"1.013747534s","start":"2025-02-18T01:55:37.807438Z","end":"2025-02-18T01:55:38.821186Z","steps":["trace[589542323] 'process raft request'  (duration: 165.982094ms)","trace[589542323] 'compare'  (duration: 182.89139ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:55:38.820621Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T01:55:37.795444Z","time spent":"593.869922ms","remote":"127.0.0.1:48574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:5100 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-02-18T01:55:38.855352Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T01:55:37.807416Z","time spent":"1.04772114s","remote":"127.0.0.1:48574","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:5101 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-02-18T01:55:38.861471Z","caller":"traceutil/trace.go:171","msg":"trace[575920728] linearizableReadLoop","detail":"{readStateIndex:6302; appliedIndex:6301; }","duration":"264.474409ms","start":"2025-02-18T01:55:38.556525Z","end":"2025-02-18T01:55:38.820999Z","steps":["trace[575920728] 'read index received'  (duration: 40.323µs)","trace[575920728] 'applied index is now lower than readState.Index'  (duration: 264.432382ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:55:38.875588Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"293.104713ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:55:38.875642Z","caller":"traceutil/trace.go:171","msg":"trace[189976493] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:5109; }","duration":"293.27156ms","start":"2025-02-18T01:55:38.582351Z","end":"2025-02-18T01:55:38.875623Z","steps":["trace[189976493] 'agreement among raft nodes before linearized reading'  (duration: 293.077532ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:55:39.088057Z","caller":"traceutil/trace.go:171","msg":"trace[1477921713] transaction","detail":"{read_only:false; response_revision:5110; number_of_response:1; }","duration":"111.109905ms","start":"2025-02-18T01:55:38.976915Z","end":"2025-02-18T01:55:39.088025Z","steps":["trace[1477921713] 'process raft request'  (duration: 110.695998ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:56:03.555121Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.225246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:56:03.555217Z","caller":"traceutil/trace.go:171","msg":"trace[570817093] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5129; }","duration":"158.371751ms","start":"2025-02-18T01:56:03.396831Z","end":"2025-02-18T01:56:03.555202Z","steps":["trace[570817093] 'range keys from in-memory index tree'  (duration: 158.127671ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:56:17.822949Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4901}
{"level":"info","ts":"2025-02-18T01:56:17.932404Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4901,"took":"104.452646ms","hash":4173789354,"current-db-size-bytes":1974272,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1974272,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-02-18T01:56:17.933150Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4173789354,"revision":4901,"compact-revision":4265}
{"level":"info","ts":"2025-02-18T01:56:43.927353Z","caller":"traceutil/trace.go:171","msg":"trace[1283753121] transaction","detail":"{read_only:false; response_revision:5162; number_of_response:1; }","duration":"115.274597ms","start":"2025-02-18T01:56:43.811963Z","end":"2025-02-18T01:56:43.927238Z","steps":["trace[1283753121] 'process raft request'  (duration: 114.934964ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:56:44.549122Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.738295ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035352373057906 > lease_revoke:<id:70cc9516bc03592a>","response":"size:29"}
{"level":"info","ts":"2025-02-18T01:56:44.549259Z","caller":"traceutil/trace.go:171","msg":"trace[687688598] linearizableReadLoop","detail":"{readStateIndex:6370; appliedIndex:6369; }","duration":"118.90271ms","start":"2025-02-18T01:56:44.430338Z","end":"2025-02-18T01:56:44.549240Z","steps":["trace[687688598] 'read index received'  (duration: 25.801µs)","trace[687688598] 'applied index is now lower than readState.Index'  (duration: 118.875147ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T01:56:44.550052Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.567674ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:56:44.550186Z","caller":"traceutil/trace.go:171","msg":"trace[1072591985] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5162; }","duration":"119.81581ms","start":"2025-02-18T01:56:44.430302Z","end":"2025-02-18T01:56:44.550118Z","steps":["trace[1072591985] 'agreement among raft nodes before linearized reading'  (duration: 118.985825ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:57:16.555405Z","caller":"traceutil/trace.go:171","msg":"trace[2072290495] linearizableReadLoop","detail":"{readStateIndex:6401; appliedIndex:6400; }","duration":"120.054507ms","start":"2025-02-18T01:57:16.435292Z","end":"2025-02-18T01:57:16.555346Z","steps":["trace[2072290495] 'read index received'  (duration: 119.448005ms)","trace[2072290495] 'applied index is now lower than readState.Index'  (duration: 605.694µs)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:57:16.555748Z","caller":"traceutil/trace.go:171","msg":"trace[676064170] transaction","detail":"{read_only:false; response_revision:5187; number_of_response:1; }","duration":"259.753991ms","start":"2025-02-18T01:57:16.295831Z","end":"2025-02-18T01:57:16.555585Z","steps":["trace[676064170] 'process raft request'  (duration: 258.984966ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:57:16.556072Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.702917ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:57:16.556199Z","caller":"traceutil/trace.go:171","msg":"trace[864520793] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5187; }","duration":"120.941445ms","start":"2025-02-18T01:57:16.435243Z","end":"2025-02-18T01:57:16.556185Z","steps":["trace[864520793] 'agreement among raft nodes before linearized reading'  (duration: 120.419365ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:57:36.961668Z","caller":"traceutil/trace.go:171","msg":"trace[1797499821] transaction","detail":"{read_only:false; response_revision:5203; number_of_response:1; }","duration":"112.413351ms","start":"2025-02-18T01:57:36.849208Z","end":"2025-02-18T01:57:36.961621Z","steps":["trace[1797499821] 'process raft request'  (duration: 111.571877ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:57:43.233610Z","caller":"traceutil/trace.go:171","msg":"trace[479181509] transaction","detail":"{read_only:false; response_revision:5209; number_of_response:1; }","duration":"195.743043ms","start":"2025-02-18T01:57:43.037851Z","end":"2025-02-18T01:57:43.233594Z","steps":["trace[479181509] 'process raft request'  (duration: 195.484158ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T01:57:43.514545Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"169.544778ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T01:57:43.514702Z","caller":"traceutil/trace.go:171","msg":"trace[1187838737] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5209; }","duration":"169.919156ms","start":"2025-02-18T01:57:43.344755Z","end":"2025-02-18T01:57:43.514674Z","steps":["trace[1187838737] 'range keys from in-memory index tree'  (duration: 169.474088ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T01:58:29.266530Z","caller":"traceutil/trace.go:171","msg":"trace[1059753874] transaction","detail":"{read_only:false; response_revision:5245; number_of_response:1; }","duration":"183.248065ms","start":"2025-02-18T01:58:29.083240Z","end":"2025-02-18T01:58:29.266488Z","steps":["trace[1059753874] 'process raft request'  (duration: 88.270715ms)","trace[1059753874] 'compare'  (duration: 94.834052ms)"],"step_count":2}
{"level":"info","ts":"2025-02-18T01:58:29.826947Z","caller":"traceutil/trace.go:171","msg":"trace[1361603810] transaction","detail":"{read_only:false; response_revision:5246; number_of_response:1; }","duration":"156.978066ms","start":"2025-02-18T01:58:29.669950Z","end":"2025-02-18T01:58:29.826928Z","steps":["trace[1361603810] 'process raft request'  (duration: 156.620149ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T02:01:18.130721Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5140}
{"level":"warn","ts":"2025-02-18T02:01:18.470363Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"230.325057ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035352373059228 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:5140 > ","response":"size:5"}
{"level":"info","ts":"2025-02-18T02:01:18.475037Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5140,"took":"283.954341ms","hash":2447173437,"current-db-size-bytes":1974272,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1454080,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-02-18T02:01:18.475210Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2447173437,"revision":5140,"compact-revision":4901}
{"level":"info","ts":"2025-02-18T02:01:18.475432Z","caller":"traceutil/trace.go:171","msg":"trace[1448532816] compact","detail":"{revision:5140; response_revision:5380; }","duration":"626.062695ms","start":"2025-02-18T02:01:17.848876Z","end":"2025-02-18T02:01:18.474939Z","steps":["trace[1448532816] 'process raft request'  (duration: 51.425555ms)","trace[1448532816] 'check and update compact revision'  (duration: 229.853555ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T02:01:18.475667Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-18T02:01:17.848845Z","time spent":"626.764508ms","remote":"127.0.0.1:48330","response type":"/etcdserverpb.KV/Compact","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2025-02-18T02:01:27.717840Z","caller":"traceutil/trace.go:171","msg":"trace[1668088780] transaction","detail":"{read_only:false; response_revision:5388; number_of_response:1; }","duration":"156.194299ms","start":"2025-02-18T02:01:27.561629Z","end":"2025-02-18T02:01:27.717824Z","steps":["trace[1668088780] 'process raft request'  (duration: 156.087657ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T02:02:24.306664Z","caller":"traceutil/trace.go:171","msg":"trace[1528941306] transaction","detail":"{read_only:false; response_revision:5432; number_of_response:1; }","duration":"195.906117ms","start":"2025-02-18T02:02:24.110740Z","end":"2025-02-18T02:02:24.306646Z","steps":["trace[1528941306] 'process raft request'  (duration: 195.696331ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-18T02:02:24.472205Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.16932ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-18T02:02:24.472300Z","caller":"traceutil/trace.go:171","msg":"trace[1904598767] linearizableReadLoop","detail":"{readStateIndex:6709; appliedIndex:6707; }","duration":"147.919146ms","start":"2025-02-18T02:02:24.298361Z","end":"2025-02-18T02:02:24.446280Z","steps":["trace[1904598767] 'read index received'  (duration: 8.09152ms)","trace[1904598767] 'applied index is now lower than readState.Index'  (duration: 139.825124ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-18T02:02:24.472700Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.331843ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-02-18T02:02:24.473160Z","caller":"traceutil/trace.go:171","msg":"trace[2047679754] range","detail":"{range_begin:/registry/endpointslices/; range_end:/registry/endpointslices0; response_count:0; response_revision:5432; }","duration":"170.417898ms","start":"2025-02-18T02:02:24.302342Z","end":"2025-02-18T02:02:24.472760Z","steps":["trace[2047679754] 'agreement among raft nodes before linearized reading'  (duration: 170.281225ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T02:02:24.473159Z","caller":"traceutil/trace.go:171","msg":"trace[189935473] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5432; }","duration":"174.030661ms","start":"2025-02-18T02:02:24.298298Z","end":"2025-02-18T02:02:24.472328Z","steps":["trace[189935473] 'agreement among raft nodes before linearized reading'  (duration: 148.228643ms)"],"step_count":1}
{"level":"info","ts":"2025-02-18T02:02:47.569262Z","caller":"traceutil/trace.go:171","msg":"trace[1846907450] transaction","detail":"{read_only:false; response_revision:5450; number_of_response:1; }","duration":"121.212122ms","start":"2025-02-18T02:02:47.447946Z","end":"2025-02-18T02:02:47.569158Z","steps":["trace[1846907450] 'process raft request'  (duration: 103.735502ms)","trace[1846907450] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:670; } (duration: 16.744763ms)"],"step_count":2}


==> kernel <==
 02:06:07 up  1:52,  0 users,  load average: 0.55, 0.39, 0.38
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [05158569c42e] <==
I0218 01:46:24.748373       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0218 01:46:24.748374       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0218 01:46:24.748394       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0218 01:46:24.748712       1 secure_serving.go:213] Serving securely on [::]:8443
I0218 01:46:24.748847       1 controller.go:78] Starting OpenAPI AggregationController
I0218 01:46:24.749143       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0218 01:46:24.749921       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0218 01:46:24.750194       1 controller.go:119] Starting legacy_token_tracking_controller
I0218 01:46:24.750256       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0218 01:46:24.751492       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0218 01:46:24.751898       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0218 01:46:24.752260       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0218 01:46:24.766608       1 aggregator.go:169] waiting for initial CRD sync...
I0218 01:46:24.773010       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0218 01:46:24.800803       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0218 01:46:24.800843       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0218 01:46:24.801157       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0218 01:46:24.801823       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0218 01:46:24.802044       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0218 01:46:24.802312       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0218 01:46:24.802734       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0218 01:46:24.802875       1 establishing_controller.go:81] Starting EstablishingController
I0218 01:46:24.802142       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0218 01:46:24.803447       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0218 01:46:24.802243       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0218 01:46:24.803497       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0218 01:46:24.804115       1 local_available_controller.go:156] Starting LocalAvailability controller
I0218 01:46:24.804190       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0218 01:46:24.804899       1 controller.go:142] Starting OpenAPI controller
I0218 01:46:24.805079       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0218 01:46:24.805148       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0218 01:46:24.805550       1 controller.go:90] Starting OpenAPI V3 controller
I0218 01:46:24.805774       1 crd_finalizer.go:269] Starting CRDFinalizer
I0218 01:46:24.806655       1 naming_controller.go:294] Starting NamingConditionController
I0218 01:46:25.117253       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0218 01:46:25.117521       1 aggregator.go:171] initial CRD sync complete...
I0218 01:46:25.117996       1 autoregister_controller.go:144] Starting autoregister controller
I0218 01:46:25.118015       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0218 01:46:25.120244       1 cache.go:39] Caches are synced for LocalAvailability controller
I0218 01:46:25.121261       1 shared_informer.go:320] Caches are synced for configmaps
I0218 01:46:25.122579       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0218 01:46:25.123172       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0218 01:46:25.133698       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0218 01:46:25.211792       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0218 01:46:25.212746       1 shared_informer.go:320] Caches are synced for node_authorizer
I0218 01:46:25.212779       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0218 01:46:25.212788       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0218 01:46:25.214036       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0218 01:46:25.214188       1 policy_source.go:240] refreshing policies
I0218 01:46:25.219575       1 cache.go:39] Caches are synced for autoregister controller
I0218 01:46:25.220248       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0218 01:46:25.414564       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0218 01:46:25.921017       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0218 01:46:26.022147       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0218 01:46:30.531794       1 controller.go:615] quota admission added evaluator for: endpoints
I0218 01:46:31.924962       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0218 01:46:31.936530       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0218 01:46:32.032317       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0218 01:46:32.124060       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0218 02:04:36.855581       1 alloc.go:330] "allocated clusterIPs" service="default/nodejs-app" clusterIPs={"IPv4":"10.109.104.129"}


==> kube-apiserver [a23503d0bd15] <==
W0218 01:45:06.737789       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:06.739419       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:06.801006       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:08.893634       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:08.989643       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.083033       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.267893       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.308461       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.397713       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.418397       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.451058       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.511762       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.680960       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.689873       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.708787       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.714488       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.727462       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.727464       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.783791       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.818823       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.853715       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.861924       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.910252       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.933094       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.934544       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:09.973031       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.045762       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.085110       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.091815       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.114334       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.176288       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.197315       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.207610       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.214363       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.245884       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.289115       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.317253       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.317335       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.371061       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.405749       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.421792       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.503065       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.540044       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.542561       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.549326       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.573492       1 logging.go:55] [core] [Channel #18 SubChannel #19]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.587048       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.629918       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.708391       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.718042       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.767031       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.779318       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.789325       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.865803       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.891845       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.892712       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:10.896489       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:11.419586       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:11.419616       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0218 01:45:11.419586       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [aa955594e560] <==
I0217 23:49:49.844414       1 shared_informer.go:320] Caches are synced for service account
I0217 23:49:49.855579       1 shared_informer.go:320] Caches are synced for deployment
I0217 23:49:49.857516       1 shared_informer.go:320] Caches are synced for disruption
I0217 23:49:49.860984       1 shared_informer.go:320] Caches are synced for resource quota
I0217 23:49:49.861006       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0217 23:49:49.863345       1 shared_informer.go:320] Caches are synced for job
I0217 23:49:49.863399       1 shared_informer.go:320] Caches are synced for daemon sets
I0217 23:49:49.863414       1 shared_informer.go:320] Caches are synced for endpoint
I0217 23:49:49.863377       1 shared_informer.go:320] Caches are synced for TTL
I0217 23:49:49.863823       1 shared_informer.go:320] Caches are synced for TTL after finished
I0217 23:49:49.866548       1 shared_informer.go:320] Caches are synced for node
I0217 23:49:49.866628       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0217 23:49:49.866664       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0217 23:49:49.866675       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0217 23:49:49.866682       1 shared_informer.go:320] Caches are synced for cidrallocator
I0217 23:49:49.873016       1 shared_informer.go:320] Caches are synced for HPA
I0217 23:49:49.877204       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0217 23:49:49.877280       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0217 23:49:49.877342       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0217 23:49:49.879976       1 shared_informer.go:320] Caches are synced for garbage collector
I0217 23:49:50.958986       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0217 23:49:51.193345       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="467.181136ms"
I0217 23:49:51.365067       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="171.633726ms"
I0217 23:49:51.365349       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="102.118µs"
I0217 23:49:51.441051       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="71.426µs"
I0217 23:49:53.876695       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="112.47µs"
I0217 23:49:56.925531       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0217 23:50:18.898110       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="12.289699ms"
I0217 23:50:18.898209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="45.822µs"
I0217 23:56:35.671583       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:01:41.778995       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:06:46.680834       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:11:53.084969       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:16:57.938594       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:22:04.135767       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:53:14.112240       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0218 00:53:23.750961       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0218 00:53:23.902382       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0218 00:58:21.388075       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 00:59:29.929218       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nodejs-app", UID:"d5fbc829-3919-49e2-8950-fa18a2ef410e", APIVersion:"v1", ResourceVersion:"2504", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/nodejs-app: failed to create EndpointSlice for Service default/nodejs-app: Unauthorized
I0218 00:59:29.930024       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/nodejs-app" err="failed to create EndpointSlice for Service default/nodejs-app: Unauthorized"
I0218 00:59:30.135376       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="77.384136ms"
E0218 00:59:30.135847       1 replica_set.go:560] "Unhandled Error" err="sync \"default/nodejs-app-8c5d666f9\" failed with Unauthorized" logger="UnhandledError"
I0218 00:59:30.224343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="86.460022ms"
I0218 00:59:30.259689       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="35.251209ms"
I0218 00:59:30.259905       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="130.351µs"
I0218 00:59:30.272418       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="885.9µs"
I0218 00:59:30.382097       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="130.335µs"
I0218 01:02:01.395946       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="36.582731ms"
I0218 01:02:01.397776       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="57.714µs"
I0218 01:02:25.764314       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:06:40.547052       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:11:46.635952       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:15:51.760801       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-5cdvm" approvedExpiration="1h0m0s"
I0218 01:16:53.372524       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:22:00.407232       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:27:06.291963       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:32:12.213183       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:37:17.872021       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:44:41.487684       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="68.282µs"


==> kube-controller-manager [e26edca000ca] <==
I0218 01:46:31.339511       1 shared_informer.go:320] Caches are synced for service account
I0218 01:46:31.339574       1 shared_informer.go:320] Caches are synced for PV protection
I0218 01:46:31.411638       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:46:31.418527       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0218 01:46:31.425108       1 shared_informer.go:320] Caches are synced for TTL
I0218 01:46:31.512492       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0218 01:46:31.512702       1 shared_informer.go:320] Caches are synced for expand
I0218 01:46:31.512737       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0218 01:46:31.513984       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0218 01:46:31.514029       1 shared_informer.go:320] Caches are synced for TTL after finished
I0218 01:46:31.520981       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0218 01:46:31.526246       1 shared_informer.go:320] Caches are synced for cronjob
I0218 01:46:31.526297       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0218 01:46:31.526320       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0218 01:46:31.526582       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0218 01:46:31.539499       1 shared_informer.go:320] Caches are synced for deployment
I0218 01:46:31.610956       1 shared_informer.go:320] Caches are synced for job
I0218 01:46:31.610956       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0218 01:46:31.610971       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0218 01:46:31.613682       1 shared_informer.go:320] Caches are synced for attach detach
I0218 01:46:31.616001       1 shared_informer.go:320] Caches are synced for persistent volume
I0218 01:46:31.617636       1 shared_informer.go:320] Caches are synced for disruption
I0218 01:46:31.621137       1 shared_informer.go:320] Caches are synced for stateful set
I0218 01:46:31.621537       1 shared_informer.go:320] Caches are synced for taint
I0218 01:46:31.625979       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0218 01:46:31.626030       1 shared_informer.go:320] Caches are synced for GC
I0218 01:46:31.626290       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0218 01:46:31.627318       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0218 01:46:31.626429       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0218 01:46:31.631275       1 shared_informer.go:320] Caches are synced for ReplicationController
I0218 01:46:31.632472       1 shared_informer.go:320] Caches are synced for HPA
I0218 01:46:31.632510       1 shared_informer.go:320] Caches are synced for daemon sets
I0218 01:46:31.634432       1 shared_informer.go:320] Caches are synced for PVC protection
I0218 01:46:31.634557       1 shared_informer.go:320] Caches are synced for ephemeral
I0218 01:46:31.634988       1 shared_informer.go:320] Caches are synced for endpoint
I0218 01:46:31.637921       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0218 01:46:31.713212       1 shared_informer.go:320] Caches are synced for resource quota
I0218 01:46:31.723752       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0218 01:46:31.727094       1 shared_informer.go:320] Caches are synced for resource quota
I0218 01:46:31.727794       1 shared_informer.go:320] Caches are synced for crt configmap
I0218 01:46:31.729424       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0218 01:46:31.930334       1 shared_informer.go:320] Caches are synced for garbage collector
I0218 01:46:32.012686       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="398.558454ms"
I0218 01:46:32.012945       1 shared_informer.go:320] Caches are synced for garbage collector
I0218 01:46:32.013045       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0218 01:46:32.013074       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0218 01:46:32.020206       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="3.394549ms"
I0218 01:46:34.222560       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="1.355086ms"
I0218 01:47:07.008615       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="493.447897ms"
I0218 01:47:07.028814       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="156.242µs"
I0218 01:53:15.261208       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 01:58:21.717009       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 02:03:28.231068       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0218 02:04:36.954833       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="120.801373ms"
I0218 02:04:36.967386       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="12.128887ms"
I0218 02:04:36.967519       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="57.522µs"
I0218 02:04:37.098290       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="69.713µs"
I0218 02:04:37.159694       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="88.859µs"
I0218 02:04:42.124765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="16.674244ms"
I0218 02:04:42.124870       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-8c5d666f9" duration="35.815µs"


==> kube-proxy [256647b30d91] <==
I0217 23:49:53.916612       1 server_linux.go:66] "Using iptables proxy"
I0217 23:49:54.324099       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0217 23:49:54.324277       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0217 23:49:54.353837       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0217 23:49:54.353926       1 server_linux.go:170] "Using iptables Proxier"
I0217 23:49:54.363536       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0217 23:49:54.376977       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0217 23:49:54.387929       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0217 23:49:54.389512       1 server.go:497] "Version info" version="v1.32.0"
I0217 23:49:54.389570       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0217 23:49:54.403723       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0217 23:49:54.419167       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0217 23:49:54.423121       1 config.go:199] "Starting service config controller"
I0217 23:49:54.423248       1 config.go:105] "Starting endpoint slice config controller"
I0217 23:49:54.423789       1 config.go:329] "Starting node config controller"
I0217 23:49:54.424745       1 shared_informer.go:313] Waiting for caches to sync for service config
I0217 23:49:54.424825       1 shared_informer.go:313] Waiting for caches to sync for node config
I0217 23:49:54.424747       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0217 23:49:54.525309       1 shared_informer.go:320] Caches are synced for service config
I0217 23:49:54.525388       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0217 23:49:54.525373       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [c58260b7eed3] <==
I0218 01:46:33.041212       1 server_linux.go:66] "Using iptables proxy"
I0218 01:46:33.889671       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0218 01:46:33.892290       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0218 01:46:33.957557       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0218 01:46:33.957679       1 server_linux.go:170] "Using iptables Proxier"
I0218 01:46:33.968715       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0218 01:46:33.993529       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0218 01:46:34.007280       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0218 01:46:34.010719       1 server.go:497] "Version info" version="v1.32.0"
I0218 01:46:34.010858       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0218 01:46:34.036480       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0218 01:46:34.067218       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0218 01:46:34.083990       1 config.go:199] "Starting service config controller"
I0218 01:46:34.084838       1 config.go:105] "Starting endpoint slice config controller"
I0218 01:46:34.085747       1 shared_informer.go:313] Waiting for caches to sync for service config
I0218 01:46:34.085794       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0218 01:46:34.089055       1 config.go:329] "Starting node config controller"
I0218 01:46:34.089123       1 shared_informer.go:313] Waiting for caches to sync for node config
I0218 01:46:34.210410       1 shared_informer.go:320] Caches are synced for service config
I0218 01:46:34.210432       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0218 01:46:34.210410       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [9b102c1810d1] <==
E0217 23:49:41.626372       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.626378       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0217 23:49:41.626407       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.626517       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:41.626561       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.626698       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0217 23:49:41.626825       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.626894       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0217 23:49:41.626963       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.626984       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0217 23:49:41.625646       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0217 23:49:41.627007       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0217 23:49:41.627010       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0217 23:49:41.627024       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.625646       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0217 23:49:41.627045       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:41.627046       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0217 23:49:41.627081       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.625697       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:41.627050       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0217 23:49:41.627218       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:41.625758       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0217 23:49:41.627241       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.439070       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:42.439148       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.453185       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:42.453263       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.464026       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0217 23:49:42.464100       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.497015       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:42.497073       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.499389       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0217 23:49:42.499472       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.581276       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0217 23:49:42.581343       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.651270       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:42.651355       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.724810       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0217 23:49:42.724867       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.760958       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0217 23:49:42.761038       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.830415       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0217 23:49:42.830494       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:42.849132       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0217 23:49:42.849216       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:43.012129       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0217 23:49:43.012209       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0217 23:49:43.044651       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0217 23:49:43.044724       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0217 23:49:43.131919       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0217 23:49:43.132148       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0217 23:49:43.147845       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0217 23:49:43.147913       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0217 23:49:43.204305       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0217 23:49:43.204389       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0217 23:49:45.723453       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0218 01:45:01.327156       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0218 01:45:01.319168       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0218 01:45:01.321129       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0218 01:45:01.644205       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [9d2f8fe6fea5] <==
I0218 01:46:15.961349       1 serving.go:386] Generated self-signed cert in-memory
W0218 01:46:25.021274       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0218 01:46:25.021342       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0218 01:46:25.021364       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0218 01:46:25.021405       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0218 01:46:25.337281       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0218 01:46:25.337376       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0218 01:46:25.428963       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0218 01:46:25.430127       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0218 01:46:25.432418       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0218 01:46:25.435021       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0218 01:46:25.538307       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 18 01:46:16 minikube kubelet[1617]: E0218 01:46:16.431822    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:16 minikube kubelet[1617]: E0218 01:46:16.431963    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:16 minikube kubelet[1617]: E0218 01:46:16.432278    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:16 minikube kubelet[1617]: E0218 01:46:16.432327    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:17 minikube kubelet[1617]: E0218 01:46:17.449868    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:20 minikube kubelet[1617]: E0218 01:46:20.122417    1617 eviction_manager.go:292] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Feb 18 01:46:21 minikube kubelet[1617]: E0218 01:46:21.329487    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:22 minikube kubelet[1617]: E0218 01:46:22.012485    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:22 minikube kubelet[1617]: E0218 01:46:22.558953    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:25 minikube kubelet[1617]: E0218 01:46:24.931107    1617 kubelet.go:3196] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.122898    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.421205    1617 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.421664    1617 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.421720    1617 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.425097    1617 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.528709    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.625451    1617 apiserver.go:52] "Watching apiserver"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.634741    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: E0218 01:46:25.919562    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: E0218 01:46:25.919683    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.920053    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: E0218 01:46:25.919791    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.925480    1617 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.928645    1617 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/71aee8b0-9105-4f6f-9b87-18b6b573a6d5-lib-modules\") pod \"kube-proxy-prqtc\" (UID: \"71aee8b0-9105-4f6f-9b87-18b6b573a6d5\") " pod="kube-system/kube-proxy-prqtc"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.929065    1617 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/925e848f-1ffe-4bb2-b862-1d319f191ce5-tmp\") pod \"storage-provisioner\" (UID: \"925e848f-1ffe-4bb2-b862-1d319f191ce5\") " pod="kube-system/storage-provisioner"
Feb 18 01:46:25 minikube kubelet[1617]: I0218 01:46:25.929206    1617 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/71aee8b0-9105-4f6f-9b87-18b6b573a6d5-xtables-lock\") pod \"kube-proxy-prqtc\" (UID: \"71aee8b0-9105-4f6f-9b87-18b6b573a6d5\") " pod="kube-system/kube-proxy-prqtc"
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.039942    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Feb 18 01:46:26 minikube kubelet[1617]: I0218 01:46:26.040723    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.112578    1617 projected.go:288] Couldn't get configMap default/kube-root-ca.crt: object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.112645    1617 projected.go:194] Error preparing data for projected volume kube-api-access-kw5vt for pod default/nodejs-app-8c5d666f9-vr8vw: object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.126125    1617 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt podName:f28dcb08-8b09-49b5-a8f0-88fe0efdb166 nodeName:}" failed. No retries permitted until 2025-02-18 01:46:26.623887346 +0000 UTC m=+17.899011102 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-kw5vt" (UniqueName: "kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt") pod "nodejs-app-8c5d666f9-vr8vw" (UID: "f28dcb08-8b09-49b5-a8f0-88fe0efdb166") : object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.215838    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Feb 18 01:46:26 minikube kubelet[1617]: I0218 01:46:26.215895    1617 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.333679    1617 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.715432    1617 projected.go:288] Couldn't get configMap default/kube-root-ca.crt: object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.715474    1617 projected.go:194] Error preparing data for projected volume kube-api-access-kw5vt for pod default/nodejs-app-8c5d666f9-vr8vw: object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:26 minikube kubelet[1617]: E0218 01:46:26.715553    1617 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt podName:f28dcb08-8b09-49b5-a8f0-88fe0efdb166 nodeName:}" failed. No retries permitted until 2025-02-18 01:46:27.715520162 +0000 UTC m=+18.990643912 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-kw5vt" (UniqueName: "kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt") pod "nodejs-app-8c5d666f9-vr8vw" (UID: "f28dcb08-8b09-49b5-a8f0-88fe0efdb166") : object "default"/"kube-root-ca.crt" not registered
Feb 18 01:46:27 minikube kubelet[1617]: I0218 01:46:27.517965    1617 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kw5vt\" (UniqueName: \"kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt\") pod \"f28dcb08-8b09-49b5-a8f0-88fe0efdb166\" (UID: \"f28dcb08-8b09-49b5-a8f0-88fe0efdb166\") "
Feb 18 01:46:27 minikube kubelet[1617]: I0218 01:46:27.522624    1617 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt" (OuterVolumeSpecName: "kube-api-access-kw5vt") pod "f28dcb08-8b09-49b5-a8f0-88fe0efdb166" (UID: "f28dcb08-8b09-49b5-a8f0-88fe0efdb166"). InnerVolumeSpecName "kube-api-access-kw5vt". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 18 01:46:27 minikube kubelet[1617]: I0218 01:46:27.624703    1617 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-kw5vt\" (UniqueName: \"kubernetes.io/projected/f28dcb08-8b09-49b5-a8f0-88fe0efdb166-kube-api-access-kw5vt\") on node \"minikube\" DevicePath \"\""
Feb 18 01:46:29 minikube kubelet[1617]: I0218 01:46:29.626174    1617 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bee0d13c8aac5163ab57d743991ee81ca934dfbe411d9f2eb344f0c2d26578a4"
Feb 18 01:46:31 minikube kubelet[1617]: I0218 01:46:31.917936    1617 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7ac192ba8033f067b8e8223925817acda27d4da25b625445d68be69be8b229b7"
Feb 18 01:46:32 minikube kubelet[1617]: E0218 01:46:32.144634    1617 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 18 01:46:32 minikube kubelet[1617]: E0218 01:46:32.146891    1617 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 18 01:46:33 minikube kubelet[1617]: I0218 01:46:33.742601    1617 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f28dcb08-8b09-49b5-a8f0-88fe0efdb166" path="/var/lib/kubelet/pods/f28dcb08-8b09-49b5-a8f0-88fe0efdb166/volumes"
Feb 18 01:46:42 minikube kubelet[1617]: E0218 01:46:42.548040    1617 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 18 01:46:42 minikube kubelet[1617]: E0218 01:46:42.548112    1617 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 18 01:46:52 minikube kubelet[1617]: E0218 01:46:52.576993    1617 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 18 01:46:52 minikube kubelet[1617]: E0218 01:46:52.577146    1617 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 18 01:46:54 minikube kubelet[1617]: I0218 01:46:54.483056    1617 scope.go:117] "RemoveContainer" containerID="d4cf624699f6f01760d7eca2c48f4f75d20bbe843773e2eebe105c5054bcdaf4"
Feb 18 01:46:54 minikube kubelet[1617]: I0218 01:46:54.483285    1617 scope.go:117] "RemoveContainer" containerID="dac13763c2f5baee092a9ad2dd7051badc4369ec34cffb5debeb7908998ed3d5"
Feb 18 01:46:54 minikube kubelet[1617]: E0218 01:46:54.484423    1617 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(925e848f-1ffe-4bb2-b862-1d319f191ce5)\"" pod="kube-system/storage-provisioner" podUID="925e848f-1ffe-4bb2-b862-1d319f191ce5"
Feb 18 01:47:02 minikube kubelet[1617]: E0218 01:47:02.598793    1617 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 18 01:47:02 minikube kubelet[1617]: E0218 01:47:02.598855    1617 helpers.go:851] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 18 01:47:08 minikube kubelet[1617]: I0218 01:47:08.733288    1617 scope.go:117] "RemoveContainer" containerID="dac13763c2f5baee092a9ad2dd7051badc4369ec34cffb5debeb7908998ed3d5"
Feb 18 01:47:10 minikube kubelet[1617]: I0218 01:47:10.127022    1617 scope.go:117] "RemoveContainer" containerID="87146cdefea3fbbf68451ac4c0c433bc7997cb273f51c0eb6872022f19bca3d5"
Feb 18 02:04:37 minikube kubelet[1617]: I0218 02:04:37.114030    1617 memory_manager.go:355] "RemoveStaleState removing state" podUID="f28dcb08-8b09-49b5-a8f0-88fe0efdb166" containerName="nodejs-app"
Feb 18 02:04:37 minikube kubelet[1617]: I0218 02:04:37.167009    1617 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qgsql\" (UniqueName: \"kubernetes.io/projected/2e980fbf-8cb8-4ff2-baf2-906790dab059-kube-api-access-qgsql\") pod \"nodejs-app-8c5d666f9-2w4rk\" (UID: \"2e980fbf-8cb8-4ff2-baf2-906790dab059\") " pod="default/nodejs-app-8c5d666f9-2w4rk"
Feb 18 02:04:39 minikube kubelet[1617]: I0218 02:04:39.042208    1617 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="9b94701297c34972030445247fc156d6f574d59067b605a4651f4a9c49e1b016"
Feb 18 02:04:42 minikube kubelet[1617]: I0218 02:04:42.109254    1617 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nodejs-app-8c5d666f9-2w4rk" podStartSLOduration=4.467904263 podStartE2EDuration="6.107567954s" podCreationTimestamp="2025-02-18 02:04:36 +0000 UTC" firstStartedPulling="2025-02-18 02:04:39.347239699 +0000 UTC m=+1110.796508815" lastFinishedPulling="2025-02-18 02:04:40.986903379 +0000 UTC m=+1112.436172506" observedRunningTime="2025-02-18 02:04:42.106739625 +0000 UTC m=+1113.556008754" watchObservedRunningTime="2025-02-18 02:04:42.107567954 +0000 UTC m=+1113.556837079"


==> storage-provisioner [d6f4f17edfd6] <==
I0218 01:47:09.236537       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0218 01:47:09.290365       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0218 01:47:09.291822       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0218 01:47:26.717103       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0218 01:47:26.717519       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"50717c69-524f-4624-86bf-1c3849f4707c", APIVersion:"v1", ResourceVersion:"4717", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c96ae186-36d7-41f8-8e1b-74ef2042f44a became leader
I0218 01:47:26.718782       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c96ae186-36d7-41f8-8e1b-74ef2042f44a!
I0218 01:47:26.822238       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c96ae186-36d7-41f8-8e1b-74ef2042f44a!


==> storage-provisioner [dac13763c2f5] <==
I0218 01:46:32.535698       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0218 01:46:53.626465       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

